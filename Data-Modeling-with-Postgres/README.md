# Data Modeling with Postgres

## 1. Introduction
A startup called Sparkify wants to analyze the data but they lack the infrastructure allowing easily querying their data, which resides in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.<sup>[1]</sup>

- **Song datasets**: Each file is in JSON format and contains information about a song and the artist of that song.
- **Log datasets**: Each file contains records of the activity generated by a music streaming app.

We are trying to create a streamlined ETL to enable convenient analysis.
## 2. Database Tables
### Database Schema
Star Schema is utilized to construct database, consisting of one fact table and 4 dimentional tables.


#### Fact Table
1. **songplays** - records in log data associated with song plays i.e. records with page NextSong
    - songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent

#### Dimension Tables
2. **users** - users in the app
- user_id, first_name, last_name, gender, level

3. **songs** - songs in music database
- song_id, title, artist_id, year, duration

4. **artists** - artists in music database
- artist_id, name, location, latitude, longitude

5. **time** - timestamps of records in songplays broken down into specific units
- start_time, hour, day, week, month, year, weekday


## 3. Files structure
### Markdown
- **README.md** introduction to this project and instruction on how to run the programs

### Data
- **data** contains all the json files to be processed.

### Python scripts
- **sql_queries.py** consists of SQL scripts to be executed by below wrapper
- **create_tables.py** initializes the database by droping existing tables and creating tables.
- **etl.py** processes JSON files from song_data and log_data and loads the records into tables. 

### Jupyter notebook
- **test.ipynb** checks the header of each table created
- **etl.ipynb** helper for developing etl.py
- **run_all.ipynb** helper to launch all python scripts

## 4. Steps to run

### Method 1: Run in console
First execute:
 ```
%run create_tables.py
```
Then:
 ```
%run etl.py
```
### Method 2: Run in Jupyter Notebook
Run all cells in below notebook:
```
run_all.ipynb
```


[1]Udacity Project Introduction: Data Modeling with Postgres

