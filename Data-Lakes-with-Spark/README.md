# Data Lake

## 1. Introduction
A startup called Sparkify wants to analyze the data but they lack the infrastructure allowing easily querying their data, which resides in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.[1]

- **Song datasets**: Each file is in JSON format and contains information about a song and the artist of that song.
- **Log datasets**: Each file contains records of the activity generated by a music streaming app.

We are trying to create a streamlined ETL to enable convenient analysis. The ETL pipeline extracts their data from S3, processes them using Spark, and loads the data back into S3 as a set of dimensional tables. This will allow their analytics team to continue finding insights in what songs their users are listening to.
## 2. Database Tables
### Database Schema
Star Schema is utilized to construct database, consisting of one fact table and 4 dimentional tables.


#### Fact Table
1. **songplays** - records in log data associated with song plays i.e. records with page NextSong
    - songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent

#### Dimension Tables
2. **users** - users in the app
- user_id, first_name, last_name, gender, level

3. **songs** - songs in music database
- song_id, title, artist_id, year, duration

4. **artists** - artists in music database
- artist_id, name, location, latitude, longitude

5. **time** - timestamps of records in songplays broken down into specific units
- start_time, hour, day, week, month, year, weekday


## 3. Files structure
### Markdown
- **README.md** introduction to this project and instruction on how to run the programs

### Config
- **dl.cfg** contains parameters for setting up AWS access.

### Python scripts
- **etl.py** processes JSON files from song_data and log_data and loads the records into tables. 

[1]Udacity Project Introduction: Data Lake

